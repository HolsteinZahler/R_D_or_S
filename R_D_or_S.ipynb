{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Republican, Democrat, or Sesame Tweet Predictor\n",
    "\n",
    "This notebook outputs a Stocastic Gradient Decent model for predicting the pollitical party of a tweeter.  Specifically, it outputs the following files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Herman D\n",
      "[nltk_data]     Schaumburg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Herman D\n",
      "[nltk_data]     Schaumburg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Tools to remove stopwords from tweets\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up stop words and defining functions.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def fix_party_code(pc):\n",
    "    if pc == 100:\n",
    "        return 'D'\n",
    "    elif pc == 200:\n",
    "        return 'R'\n",
    "    else:\n",
    "        return 'S'\n",
    "#    return int(pc/100-1)\n",
    "def list_tostring(input_list):\n",
    "    return ' '.join(input_list)\n",
    "\n",
    "def remove_stopwords(input_list):\n",
    "    return [w for w in input_list if not w in stop_words]\n",
    "\n",
    "def clean_tweets(input_df):\n",
    "    input_df['party_code'] = input_df['party_code'].apply(fix_party_code)\n",
    "    input_df['text'] = input_df['text'].apply(word_tokenize)\n",
    "    input_df['text'] = input_df['text'].apply(remove_stopwords)\n",
    "    input_df['text'] = input_df['text'].apply(list_tostring)    \n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding R and D tweets to training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>bioguide_id</th>\n",
       "      <th>mem_name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state_abbr</th>\n",
       "      <th>party_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37007274</td>\n",
       "      <td>Y000033</td>\n",
       "      <td>Don Young</td>\n",
       "      <td>repdonyoung</td>\n",
       "      <td>House</td>\n",
       "      <td>AK</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2559398984</td>\n",
       "      <td>Y000033</td>\n",
       "      <td>Don Young</td>\n",
       "      <td>DonYoungAK</td>\n",
       "      <td>House</td>\n",
       "      <td>AK</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2253968388</td>\n",
       "      <td>B001289</td>\n",
       "      <td>Bradley Byrne</td>\n",
       "      <td>RepByrne</td>\n",
       "      <td>House</td>\n",
       "      <td>AL</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42481696</td>\n",
       "      <td>B001289</td>\n",
       "      <td>Bradley Byrne</td>\n",
       "      <td>BradleyByrne</td>\n",
       "      <td>House</td>\n",
       "      <td>AL</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2861616083</td>\n",
       "      <td>P000609</td>\n",
       "      <td>Gary Palmer</td>\n",
       "      <td>USRepGaryPalmer</td>\n",
       "      <td>House</td>\n",
       "      <td>AL</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_id bioguide_id       mem_name      screen_name chamber state_abbr  \\\n",
       "0    37007274     Y000033      Don Young      repdonyoung   House         AK   \n",
       "1  2559398984     Y000033      Don Young       DonYoungAK   House         AK   \n",
       "2  2253968388     B001289  Bradley Byrne         RepByrne   House         AL   \n",
       "3    42481696     B001289  Bradley Byrne     BradleyByrne   House         AL   \n",
       "4  2861616083     P000609    Gary Palmer  USRepGaryPalmer   House         AL   \n",
       "\n",
       "   party_code  \n",
       "0         200  \n",
       "1         200  \n",
       "2         200  \n",
       "3         200  \n",
       "4         200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv with party affiliations\n",
    "party_code_df = pd.read_csv(\"partycode.csv\")\n",
    "party_code_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dict = party_code_df[['party_code','account_id']].set_index('account_id').to_dict()\n",
    "party_dict = party_dict['party_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data consists of 1000 tweets per party(R,D,S).\n"
     ]
    }
   ],
   "source": [
    "# Specify dates for training data\n",
    "dates = pd.date_range(start='7/01/2020', end='7/10/2020')\n",
    "dates = [str(date)[0:10] for date in dates]\n",
    "\n",
    "# Specify number of samples per day per party\n",
    "# date_samples = 1000\n",
    "date_samples = 100\n",
    "print('Training data consists of '+str(date_samples*1*len(dates))+' tweets per party(R,D,S).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-07-01'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = dates[0]\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-01\n",
      "2020-07-02\n",
      "2020-07-03\n",
      "2020-07-04\n",
      "2020-07-05\n",
      "2020-07-06\n",
      "2020-07-07\n",
      "2020-07-08\n",
      "2020-07-09\n",
      "2020-07-10\n"
     ]
    }
   ],
   "source": [
    "# Create empty dataframe\n",
    "train_df = pd.DataFrame(columns=['text','party_code'])\n",
    "\n",
    "for date in dates:\n",
    "    date_str = date\n",
    "    print(date)\n",
    "    example_tweets = pd.read_json(\"congresstweets/data/\"+date_str+\".json\")\n",
    "    example_tweets[\"party_code\"] = example_tweets[\"user_id\"]\n",
    "    example_tweets = example_tweets.replace({\"party_code\":party_dict})\n",
    "    date_train_data = example_tweets[(example_tweets['party_code']!=100)|(example_tweets['party_code']!=200)][['text','party_code']]\n",
    "    # Rearrange rows\n",
    "    date_train_data = date_train_data.sample(frac=1)\n",
    "    date_train_data_R=date_train_data[date_train_data['party_code']==200].head(date_samples)\n",
    "    date_train_data_D=date_train_data[date_train_data['party_code']==100].head(date_samples)\n",
    "    # Remove stop words and fix party code\n",
    "    date_train_data = clean_tweets(date_train_data_D.append(date_train_data_R, ignore_index=True))\n",
    "    #if date == date[0]:\n",
    "    #    train_df = date_train_data\n",
    "    #else:\n",
    "    train_df = train_df.append(date_train_data)#, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesame_tweets = pd.read_csv(\"sesametweets/data/sesame_tweets.csv\", usecols = ['text'])\n",
    "sesame_tweets = sesame_tweets.assign(party_code='S')\n",
    "sesame_tweets = clean_tweets(sesame_tweets)\n",
    "\n",
    "train_sesame_tweets = sesame_tweets.head(1000)\n",
    "\n",
    "train_df = train_df.append(train_sesame_tweets)\n",
    "train_df = train_df.sample(frac=1)\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system(\"mkdir data_preprocessors\")\n",
    "system(\"mkdir vectorized_data\")\n",
    "\n",
    "# Unigram Counts\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "unigram_vectorizer.fit(train_df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_preprocessors/unigram_vectorizer.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(unigram_vectorizer, 'data_preprocessors/unigram_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')\n",
    "\n",
    "X_train_unigram = unigram_vectorizer.transform(train_df['text'].values)\n",
    "save_npz('vectorized_data/X_train_unigram.npz', X_train_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_preprocessors/unigram_tf_idf_transformer.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram Tf-Idf\n",
    "\n",
    "unigram_tf_idf_transformer = TfidfTransformer()\n",
    "unigram_tf_idf_transformer.fit(X_train_unigram)\n",
    "\n",
    "dump(unigram_tf_idf_transformer, 'data_preprocessors/unigram_tf_idf_transformer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_train_unigram)\n",
    "\n",
    "save_npz('vectorized_data/X_train_unigram_tf_idf.npz', X_train_unigram_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_preprocessors/bigram_vectorizer.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram Counts\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "bigram_vectorizer.fit(train_df['text'].values)\n",
    "\n",
    "dump(bigram_vectorizer, 'data_preprocessors/bigram_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram = bigram_vectorizer.transform(train_df['text'].values)\n",
    "\n",
    "save_npz('vectorized_data/X_train_bigram.npz', X_train_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_preprocessors/bigram_tf_idf_transformer.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "\n",
    "dump(bigram_tf_idf_transformer, 'data_preprocessors/bigram_tf_idf_transformer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "\n",
    "save_npz('vectorized_data/X_train_bigram_tf_idf.npz', X_train_bigram_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.75, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 3)} ; Validation score: {round(valid_score, 3)}\\n')\n",
    "\n",
    "y_train = train_df['party_code'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Counts\n",
      "Train score: 1.0 ; Validation score: 0.795\n",
      "\n",
      "Unigram Tf-Idf\n",
      "Train score: 0.999 ; Validation score: 0.805\n",
      "\n",
      "Bigram Counts\n",
      "Train score: 1.0 ; Validation score: 0.789\n",
      "\n",
      "Bigram Tf-Idf\n",
      "Train score: 1.0 ; Validation score: 0.837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n",
    "train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n",
    "train_and_show_scores(X_train_bigram, y_train, 'Bigram Counts')\n",
    "train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Cross-Validation for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'eta0': 0.007666261216410279, 'learning_rate': 'optimal', 'loss': 'hinge'}\n",
      "Best score: 0.8360000000000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "X_train = X_train_bigram_tf_idf\n",
    "\n",
    "\n",
    "# Phase 1: loss, learning rate and initial learning rate\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "distributions = dict(\n",
    "    loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    learning_rate=['optimal', 'invscaling', 'adaptive'],\n",
    "    eta0=uniform(loc=1e-7, scale=1e-2)\n",
    ")\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=100\n",
    ")\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'alpha': 8.412554870901323e-05, 'penalty': 'l2'}\n",
      "Best score: 0.8373333333333333\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: penalty and alpha\n",
    "\n",
    "clf = SGDClassifier()\n",
    "\n",
    "distributions = dict(\n",
    "    penalty=['l1', 'l2', 'elasticnet'],\n",
    "    alpha=uniform(loc=1e-6, scale=1e-4)\n",
    ")\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50\n",
    ")\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifiers/sgd_classifier.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system(\"mkdir classifiers\")\n",
    "\n",
    "sgd_classifier = random_search_cv.best_estimator_\n",
    "\n",
    "dump(random_search_cv.best_estimator_, 'classifiers/sgd_classifier.joblib')\n",
    "\n",
    "# sgd_classifier = load('classifiers/sgd_classifier.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1319, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sesame_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecte Sesame Tweets\n",
    "num_test_tweets = 319\n",
    "test_sesame_tweets = sesame_tweets.tail(319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather test data\n",
    "# Create empty dataframe\n",
    "test_df = pd.DataFrame(columns=['text','party_code'])\n",
    "\n",
    "# Specify dates for test data\n",
    "test_dates = pd.date_range(start='8/2/2020', end='8/3/2020')\n",
    "test_dates = [str(date)[0:10] for date in dates]\n",
    "\n",
    "for date in test_dates:\n",
    "    date_str = date\n",
    "    example_tweets = pd.read_json(\"congresstweets/data/\"+date_str+\".json\")\n",
    "    example_tweets[\"party_code\"] = example_tweets[\"user_id\"]\n",
    "    example_tweets = example_tweets.replace({\"party_code\":party_dict})\n",
    "    date_test_data = example_tweets[(example_tweets['party_code']!=100)|(example_tweets['party_code']!=200)][['text','party_code']]\n",
    "    date_test_data = clean_tweets(date_test_data)\n",
    "    #if date == date[0]:\n",
    "    #    train_df = date_train_data\n",
    "    #else:\n",
    "    test_df = test_df.append(date_test_data)#, ignore_index=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True).head(2*num_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.append(test_sesame_tweets)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7189132706374086\n"
     ]
    }
   ],
   "source": [
    "X_test = bigram_vectorizer.transform(test_df['text'].values)\n",
    "X_test = bigram_tf_idf_transformer.transform(X_test)\n",
    "#X_test = unigram_tf_idf_transformer.transform(X_test)\n",
    "y_test = test_df['party_code'].values\n",
    "\n",
    "score = sgd_classifier.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mitch McConnell\n",
    "in_text = \"This new bill strengthens some of the most important parts of the CARES Act, especially the popular Paycheck Protection Program which is saving millions of American jobs as we speak. And Republicans successfully kept extraneous issues out of the bill. It’s a win for the country.\"\n",
    "fun_input = list_tostring(word_tokenize(in_text))\n",
    "unigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')\n",
    "X_pred = bigram_vectorizer.transform([fun_input])\n",
    "X_pred = bigram_tf_idf_transformer.transform(X_pred)\n",
    "sgd_classifier = load('classifiers/sgd_classifier.joblib')\n",
    "\n",
    "result = sgd_classifier.predict(X_pred)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chuck Schumer\n",
    "in_text = \"Amid coronavirus—Trump admin's actively working to roll back critical protections against predatory lenders We’ll fight this—we need to crack down on loan sharks preying on the most vulnerable consumers Not let them peddle interest rates as high as 800%!\"\n",
    "fun_input = list_tostring(word_tokenize(in_text))\n",
    "unigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')\n",
    "X_pred = bigram_vectorizer.transform([fun_input])\n",
    "X_pred = bigram_tf_idf_transformer.transform(X_pred)\n",
    "sgd_classifier = load('classifiers/sgd_classifier.joblib')\n",
    "\n",
    "result = sgd_classifier.predict(X_pred)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cookie Monster\n",
    "in_text = \"Ice cream truck passed by today. Why there no cookie trucks? Could be new business opportunity!\"\n",
    "fun_input = list_tostring(word_tokenize(in_text))\n",
    "unigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')\n",
    "X_pred = bigram_vectorizer.transform([fun_input])\n",
    "X_pred = bigram_tf_idf_transformer.transform(X_pred)\n",
    "sgd_classifier = load('classifiers/sgd_classifier.joblib')\n",
    "\n",
    "result = sgd_classifier.predict(X_pred)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
